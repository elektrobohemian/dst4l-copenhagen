{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Motivation\n",
    "\n",
    "* what to expect from this tutorial"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preparation\n",
    "\n",
    "The following code snippet initializes your Python run-time enviroment in order to run all of the subsequent actions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# The %... is an iPython thing, and is not part of the Python language.\n",
    "# In this case we're just telling the plotting library to draw things on\n",
    "# the notebook, instead of on a separate window.\n",
    "%matplotlib inline\n",
    "# See all the \"as ...\" contructs? They're just aliasing the package names.\n",
    "# That way we can call methods like plt.plot() instead of matplotlib.pyplot.plot().\n",
    "import numpy as np\n",
    "import scipy as sp\n",
    "import matplotlib as mpl\n",
    "import matplotlib.cm as cm\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import time\n",
    "pd.set_option('display.width', 500)\n",
    "pd.set_option('display.max_columns', 100)\n",
    "pd.set_option('display.notebook_repr_html', True)\n",
    "import seaborn as sns\n",
    "sns.set_style(\"whitegrid\")\n",
    "sns.set_context(\"poster\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A Brief Introduction to the Vector Space Model\n",
    "\n",
    "See http://nlp.stanford.edu/IR-book/ from which most of this text is taken. The text has been augmented with hyperlinks to information retrieval (IR) or machine learning (ML) concepts for the interested reader. Text parts in italics have been taken from http://nbn-resolving.de/urn/resolver.pl?urn:nbn:de:kobv:co1-opus4-35218.\n",
    "\n",
    "_The vector space model (VSM) is the most prominent algebraic retrieval model in IR. It is called algebraic because both queries and document representations are modeled in an $t$-dimensional vector space. The dimensionality $t$ is determined by the number of index terms, i.e., $t=|K|$. The determination of the similarity between a query vector and a document representation vector is solved by using methods from linear algebra, e.g., the cosine of the angle between both vectors, i.e., the cosine similarity that we will use below._\n",
    "\n",
    "_In traditional IR, the index vocabulary $\\mathbf{K}$ consists of index terms or keywords that can be used during retrieval. Only a keyword $\\mathbf{k_{j}}$ out of $\\mathbf{K}~(\\mathbf{k_{j}}\\in \\mathbf{K})$ can be used for the representation of a document. In order to indicate whether a specific index term $\\mathbf{k_{j}}$ is present in a document representation $\\mathbf{d_{i}}$, an index term weight $\\mathbf{w_{i,j}} \\geq 0$ is used. If $\\mathbf{w_{i,j}} = 0$ then $\\mathbf{d_{i}}$ does not contain $\\mathbf{k_{j}}$. Hence, a document representation has the following (vector) form: $\\mathbf{d_{i}}=(\\mathbf{w_{i,1}},\\mathbf{w_{i,2}},\\dots,\\mathbf{w_{i,j}})$._\n",
    "\n",
    "In other words, there is one axis for each index term in the vector space containing all document vectors.\n",
    "\n",
    ">This representation loses the relative ordering of the terms in each document. The documents *Mary is quicker than John* and *John is quicker than Mary* are identical in such a *[bag of words](https://en.wikipedia.org/wiki/Bag-of-words_model)* representation. the standard way of quantifying the similarity between two documents $d_1$ and $d_2$  is to compute the *[cosine similarity](https://en.wikipedia.org/wiki/Cosine_similarity)* of their vector representations $\\bar V(d_1)$ and $\\bar V(d_2)$:\n",
    "\n",
    "$$sim_{d_1,d_2} = \\frac{\\bar V(d_1) \\cdot \\bar V(d_2)}{|\\bar V(d_1)| |\\bar V(d_2)|}$$\n",
    "\n",
    ">The formula can be viewed as the [dot product](https://en.wikipedia.org/wiki/Dot_product) of the normalized versions of the two document vectors (The normalization happens in the denominator, right below the line of the fraction). \n",
    "\n",
    "This equation equals to the following if we transform the vector form into a simple algebraic evaluation:\n",
    "\n",
    "$$sim(d_{1},d_2)=\\frac{\\sum^{t}_{j=1}d_{1,j}\\cdot d_{2,j}}{\\sqrt{\\sum^{t}_{j=1}d^{2}_{1,j}\\cdot \\sum^{t}_{j=1}d^{2}_{2,j}}}$$\n",
    "\n",
    "_Obviously, $sim(d_{1},d_{2})\\rightarrow[0,1]$ holds (because of the normalization), where $1$ denotes a full match or the highest degree of similarity. This value is used to sort the retrieved documents in decreasing order to present a ranking to the user that also includes partial matches, e.g., documents that do not contain all index terms of the query. The index term weights can be calculated, e.g., by counting the occurrence of terms in a document (as we will do below) or by using the well-known [$tf\\ast idf$ formula](https://en.wikipedia.org/wiki/Tf%E2%80%93idf). The $tf\\ast idf$ formula assigns each $w_{i,j}$ with a product of the term frequency ($tf$) of $k_{j}$ in $d_{i}$ and the inverse of frequency ($idf$) of $k_{j}$ in the collection . The $idf$ expresses how rare an index term is within the collection._\n",
    "\n",
    ">Viewing a collection of $N$ documents as a collection of vectors leads to a TERM-DOCUMENT natural view of a collection as a term-document matrix: this is an $|K| \\times N$ matrix whose rows represent the $|K|$ terms (dimensions) of the $N$ columns, each of which corresponds to a document.\n",
    "\n",
    "![novel terms](terms.png)\n",
    "\n",
    ">There is a far more compelling reason to represent documents as vectors: we can also view a query as a vector. Consider the query q = _jealous gossip_. This query turns into the unit vector $\\bar V(q) = (0, 0.707, 0.707)$ on the three coordinates below. \n",
    "\n",
    "![novel terms](terms2.png)\n",
    "\n",
    ">The key idea now: to assign to each document $d_i$ a score equal to the dot product:\n",
    "\n",
    "$$\\bar V(q) \\cdot \\bar V(d_i)$$\n",
    "\n",
    "Then, you repeat this calculation for all three documents.\n",
    "\n",
    "You can visualize the outcome of this equation as follows. Basically, you are trying to measure the angle between the documents and the query to determine the relevance (or the similarity) of a document with respect to the query. \n",
    "![Vector Space Model](vsm.png)\n",
    "\n",
    ">\"Wuthering Heights\" (WH) is the top-scoring document for this query with a score of $0.509$, with \"Pride and Prejudice\" (PaP) a distant second with a score of $0.085$, and \"Sense and Sensibility\" (SaS) last with a score of $0.074$. This simple example is somewhat misleading: the number of dimensions in practice will be far larger than three: it will equal the vocabulary size $|K|$.\n",
    "\n",
    "### Working with the Vector Space Model in Python\n",
    "\n",
    "In this example, we will use the [CountVectorizer](http://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html) of the *scikit-learn* package (a ML package for Python). The *CountVectorizer* creates document vector with index term weights that reflect the frequency of a terms occurence in a document as the following example illustrates:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original documents are\n",
      "Hop on pop\n",
      "Hop off pop\n",
      "Hop Hop hop\n",
      "The vocabulary's size is: 4\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# create three different documents\n",
    "documents = ['Hop on pop', 'Hop off pop', 'Hop Hop hop']\n",
    "print \"Original documents are\\n\", '\\n'.join(documents)\n",
    "\n",
    "vectorizer = CountVectorizer(min_df=0)\n",
    "\n",
    "# call `fit` to build the vocabulary from the given documents\n",
    "vectorizer.fit(documents)\n",
    "# as there are four different terms in the documents, the vocabulary's size should be four as well\n",
    "# the str() \"casts\" the number returned by len() to a string in order to display it\n",
    "print \"The vocabulary's size is: \"+str(len(vectorizer.vocabulary_.keys())) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original documents are\n",
      "Hop on pop\n",
      "Hop off pop\n",
      "Hop Hop hop\n",
      "\n",
      "Index term weights for each document:\n",
      "Transformed text vector is \n",
      "[[1 0 1 1]\n",
      " [1 1 0 1]\n",
      " [3 0 0 0]]\n",
      "\n",
      "Words for each feature:\n",
      "[u'hop', u'off', u'on', u'pop']\n"
     ]
    }
   ],
   "source": [
    "# call `transform` to convert text to a bag of words\n",
    "x = vectorizer.transform(documents)\n",
    "\n",
    "# CountVectorizer uses a sparse array to save memory, but it's easier in this assignment to \n",
    "# convert back to a \"normal\" NumPy array\n",
    "x = x.toarray()\n",
    "\n",
    "print \"Original documents are\\n\", '\\n'.join(documents)\n",
    "print \"\\nIndex term weights for each document:\"\n",
    "print \"Transformed text vector is \\n\", x\n",
    "\n",
    "# `get_feature_names` tracks which word is associated with each column of the transformed x\n",
    "print\n",
    "print \"Words for each feature:\"\n",
    "print vectorizer.get_feature_names()\n",
    "\n",
    "# Notice that the bag of words treatment doesn't preserve information about the *order* of words, \n",
    "# just their frequency"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is easy to see that the last row represents the document *Hop Hop hop* as it has the value $3$ for the first index term weight associated with the term *hop*. Note that the *CountVectorizer* converts all characters to lowercase by default.\n",
    "\n",
    "Now, we will check which document is most relevant for the query *hop on*. For the sake of brevity, we will directly input the query in vector format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.57735027  0.          0.57735027  0.57735027]\n",
      " [ 0.57735027  0.57735027  0.          0.57735027]\n",
      " [ 1.          0.          0.          0.        ]]\n",
      "\n",
      "The most similar document has a similarity of: 0.816496580928\n",
      "It is: Hop on pop[ 0.57735027  0.          0.57735027  0.57735027]\n"
     ]
    }
   ],
   "source": [
    "from scipy.spatial.distance import cosine\n",
    "from sklearn.preprocessing import normalize\n",
    "\n",
    "query=np.array([1,0,1,0]).astype(float) # we have to use a NumPy floating point array\n",
    "x=x.astype(float) # for the documents, we need floating points as well as we want to normalize later\n",
    "x=normalize(x)\n",
    "print x\n",
    "# this variables is set to the maximum distance to find the best, i.e., the nearest, document to the query \n",
    "dist=1\n",
    "documentIndex=0\n",
    "for index,document in enumerate(x):\n",
    "    currentDist=cosine(query,document)\n",
    "    if currentDist<= dist:\n",
    "        dist=currentDist\n",
    "        documentIndex=index\n",
    "# we have to take 1-dist, because we have calculated a distance above but want to display a similarity\n",
    "# a distance is the logical counterpart of a distance and as both values are in the range of 0 and 1, we can simply invert\n",
    "# them to yield the logical counterpart\n",
    "print \n",
    "print \"The most similar document has a similarity of: \" + str(1-dist) \n",
    "print \"It is: \"+str(documents[documentIndex])+str(x[documentIndex])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Probability Theory\n",
    "_This informal introduction of probability theory is oriented on [ASH, Robert B.: Basic Probability Theory. Dover ed. Mineola, NY : Dover Publ., 2008]. It aims at summarizing classical probability theory being based on Kolmogorov's axioms of probability theory._\n",
    "\n",
    "### Basic Terminology\n",
    "_Classical probability theory can be best described with the help of an observation of a random experiment, e.g., the rolling of a die._\n",
    "\n",
    "_The **sample space** $\\Omega$ represents all possible outcomes of a random experiment , i.e., $\\Omega=\\{1,2,\\dots,6\\}$ for the rolling of a die._\n",
    "\n",
    "_**Events** are sentences about the experiment involving a number of outcomes that can be assigned with a truth value, e.g., that an even number has been rolled, $A=\\{2,4,6\\}$. Hence, $A\\subseteq\\Omega$ holds._\n",
    "\n",
    "_The **probability** of an event $P(A)$ is a **probability measure** $P$ that assigns an event $A$ a value out of $[0,1]$, i.e., $P:A\\subseteq\\Omega\\rightarrow[0,1]$, while $P(\\Omega)=1$ and $P(\\varnothing)=0$, i.e., an event that is not in $\\Omega$ cannot occur during the random experiment. Furthermore, for a probability measure countable additivity must hold. That is, for all countable sequences of disjoint events $\\{A_{i}\\}$ (see below) the following holds:_\n",
    "$$\n",
    "P(\\bigcup_{i\\geq1}A_{i})=\\sum_{i\\geq1}P(A_{i}).\n",
    "$$\n",
    "\n",
    "_To simplify the handling of $P(A)$, we commonly assign it a probability based on the frequency or chance of the event, e.g., $P(\\mbox{``1 is rolled''})=\\frac{1}{6}$._\n",
    "\n",
    "### Event Algebra \n",
    "_Being a Boolean algebra, the algebra of events and probabilities resembles the algebra of real numbers, i.e., a union ($\\cup$) corresponds to an addition while an intersection ($\\cap$) corresponds to a multiplication, if the events are independent._\n",
    " \n",
    "_Two events $A$ and $B$ are \\emph{independent} if and only if\n",
    "$$\n",
    "P(A\\cap B)=P(A)\\cdot P(B).\n",
    "$$\n",
    "_That is, the occurrence (or non-occurrence) of $A$ has no effect on the occurrence of $B$. Otherwise, we have to deal with a conditional probability (see below)._\n",
    " \n",
    "_The probability of mutually exclusive/disjoint events, i.e., $E_{i}\\cap E_{j}=\\varnothing;~\\mbox{for~} i\\neq j$, is defined as follows:_\n",
    "$$\n",
    "P(E_{1}\\cup E_{2}\\cup \\dots \\cup E_{n})=\\sum^{n}_{i=1}P(E_{i})\n",
    "$$\n",
    "_If two events are \\emph{not} mutually exclusive the following applies:_\n",
    "$$\n",
    "P(A\\cup B)=P(A)+P(B)-P(A\\cap B)\n",
    "$$\n",
    "_To conclude, let the complement be:_\n",
    "\n",
    "$$\n",
    "P(\\Omega\\backslash A)=P(\\neg A)=1-P(A)\n",
    "$$\n",
    "\n",
    "### Conditional Probabilities\n",
    "\n",
    "_If two probabilities are affected by each other, they are called \\emph{conditional}. That is, the probability of $A$ on the condition that $B$ has occurred is defined as follows:_\n",
    "$$\n",
    "P(A | B)=\\frac{P(A \\cap B)}{P(B)} \\quad\\mbox{if } P(B)\\neq 0.\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bayes' Theorem\n",
    "\n",
    "_The Bayes' theorem can be interpreted as a means to calculate the conditional probabilities of two events $A$ and $B$ by \"inverting\" them. The following transformation can become useful in the domain of IR (as well as in other fields) if certain conditional probabilities are easier to calculate than others._\n",
    "$$\n",
    "P(A | B)=\\frac{P(B | A)\\cdot P(A)}{P(B)}\n",
    "$$\n",
    "\n",
    "Further details can be found in \"Think Bayes\", a very fun O'Reilly book(online at http://www.greenteapress.com/thinkbayes/.\n",
    " \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Naive Bayes Classifier\n",
    "\n",
    "Wikipedia provides a good overview of the [_naive bayes classifier_](https://en.wikipedia.org/wiki/Naive_Bayes_classifier) and its history.\n",
    "\n",
    "Some parts and code stolen from HW3 for CS109 in 2013. \n",
    "\n",
    "\n",
    "\n",
    "$$P(c|d) \\propto P(d|c) P(c) $$\n",
    "\n",
    "$$P(d|c)  = \\prod_k P(t_k | c) $$, the conditional independence assumption.\n",
    "\n",
    "Then we see that for which c is $P(c|d)$ higher.\n",
    "\n",
    "For [floating point underflow](https://en.wikipedia.org/wiki/Arithmetic_underflow) we change the product into a sum. But we must also handle non-existent terms, we cant have 0's for them:\n",
    "\n",
    "$$P(t_k|c) = \\frac{N_{kc}+\\alpha}{N_c+\\alpha N_{feat}}$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementing the Classifier\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Rotten Tomatoes Data Set\n",
    "\n",
    "In order to build our classifier, we need to read in data. The data file is provided in CSV format in the current directory. The \"./\" is a shortcut for the current working directory in which this notebook resides. Mac or Linux user can check this by running the following cell:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/david/Documents/src/python/dst4l-copenhagen\n",
      "NaiveBayes.ipynb              all.csv                       english.stop.txt              terms2.png\n",
      "README.md                     babypython.ipynb              hamlet.txt                    vsm.png\n",
      "Teaser.ipynb                  critics.csv                   obfuscatedpython.md\n",
      "TextAnalysisScikitLearn.ipynb dst4l0.ipynb                  terms.png\n"
     ]
    }
   ],
   "source": [
    "!pwd\n",
    "!ls"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Windows users will have to run this cell:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/bin/sh: dir: command not found\r\n"
     ]
    }
   ],
   "source": [
    "!dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>critic</th>\n",
       "      <th>fresh</th>\n",
       "      <th>imdb</th>\n",
       "      <th>publication</th>\n",
       "      <th>quote</th>\n",
       "      <th>review_date</th>\n",
       "      <th>rtid</th>\n",
       "      <th>title</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Derek Adams</td>\n",
       "      <td>fresh</td>\n",
       "      <td>114709</td>\n",
       "      <td>Time Out</td>\n",
       "      <td>So ingenious in concept, design and execution ...</td>\n",
       "      <td>2009-10-04</td>\n",
       "      <td>9559</td>\n",
       "      <td>Toy story</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Richard Corliss</td>\n",
       "      <td>fresh</td>\n",
       "      <td>114709</td>\n",
       "      <td>TIME Magazine</td>\n",
       "      <td>The year's most inventive comedy.</td>\n",
       "      <td>2008-08-31</td>\n",
       "      <td>9559</td>\n",
       "      <td>Toy story</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>David Ansen</td>\n",
       "      <td>fresh</td>\n",
       "      <td>114709</td>\n",
       "      <td>Newsweek</td>\n",
       "      <td>A winning animated feature that has something ...</td>\n",
       "      <td>2008-08-18</td>\n",
       "      <td>9559</td>\n",
       "      <td>Toy story</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Leonard Klady</td>\n",
       "      <td>fresh</td>\n",
       "      <td>114709</td>\n",
       "      <td>Variety</td>\n",
       "      <td>The film sports a provocative and appealing st...</td>\n",
       "      <td>2008-06-09</td>\n",
       "      <td>9559</td>\n",
       "      <td>Toy story</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Jonathan Rosenbaum</td>\n",
       "      <td>fresh</td>\n",
       "      <td>114709</td>\n",
       "      <td>Chicago Reader</td>\n",
       "      <td>An entertaining computer-generated, hyperreali...</td>\n",
       "      <td>2008-03-10</td>\n",
       "      <td>9559</td>\n",
       "      <td>Toy story</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               critic  fresh    imdb     publication                                              quote review_date  rtid      title\n",
       "1         Derek Adams  fresh  114709        Time Out  So ingenious in concept, design and execution ...  2009-10-04  9559  Toy story\n",
       "2     Richard Corliss  fresh  114709   TIME Magazine                  The year's most inventive comedy.  2008-08-31  9559  Toy story\n",
       "3         David Ansen  fresh  114709        Newsweek  A winning animated feature that has something ...  2008-08-18  9559  Toy story\n",
       "4       Leonard Klady  fresh  114709         Variety  The film sports a provocative and appealing st...  2008-06-09  9559  Toy story\n",
       "5  Jonathan Rosenbaum  fresh  114709  Chicago Reader  An entertaining computer-generated, hyperreali...  2008-03-10  9559  Toy story"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# reads the CSV using the pandas Python package, this creates a variable called \"critics\" that will store the\n",
    "# CSV contents in form of a table\n",
    "critics = pd.read_csv('./critics.csv')\n",
    "# let's drop rows with missing quotes by using the negation operator ~\n",
    "# an alternative way of expressing this would be: critics = critics[~critics.quote.notnull()]\n",
    "critics = critics[~critics.quote.isnull()]\n",
    "# display the first rows of the table\n",
    "critics.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The *read_csv()* function is a powerful tool and can deal with a variety of CSV file formats as its [documentation](http://pandas.pydata.org/pandas-docs/stable/generated/pandas.read_csv.html) shows.\n",
    "The function returns a so-called [DataFrame](http://pandas.pydata.org/pandas-docs/stable/api.html#dataframe) which itself offers a lot of functionality as the *.head()* function used above. Another function is *.dtypes()* that displays the data types of the columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "critic         object\n",
       "fresh          object\n",
       "imdb            int64\n",
       "publication    object\n",
       "quote          object\n",
       "review_date    object\n",
       "rtid            int64\n",
       "title          object\n",
       "dtype: object"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "    critics.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Explore Your Data Set\n",
    "Typically, the first step in IR and MR is to learn more about your data, i.e., to get a feeling for it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of reviews: 15561\n",
      "There are 623 active critics. \n",
      "1921 movies were found.\n"
     ]
    }
   ],
   "source": [
    "# the len() function return the length of an object, i.e., the number of rows stored in critics coming from your CSV\n",
    "n_reviews = len(critics)\n",
    "# find the number of unique entries in the rtid column\n",
    "n_movies = critics.rtid.unique().size\n",
    "# find the number of unique entries in the critic column\n",
    "n_critics = critics.critic.unique().size\n",
    "\n",
    "\n",
    "print \"Number of reviews: %i\" % n_reviews # %i is a placeholder for the number following after the string\n",
    "print \"There are %i active critics. \" % n_critics # you can place the placeholder wherever you want\n",
    "print \"%i movies were found.\" % n_movies "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([<matplotlib.axis.YTick at 0x10c87dbd0>,\n",
       "  <matplotlib.axis.YTick at 0x10c87d510>,\n",
       "  <matplotlib.axis.YTick at 0x109aa2b10>,\n",
       "  <matplotlib.axis.YTick at 0x109e87410>,\n",
       "  <matplotlib.axis.YTick at 0x109e87b90>,\n",
       "  <matplotlib.axis.YTick at 0x109e90350>],\n",
       " <a list of 6 Text yticklabel objects>)"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAxQAAAIqCAYAAAC9hAz4AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x10cda4ed0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "df = critics.copy()\n",
    "df['fresh'] = df.fresh == 'fresh'\n",
    "grp = df.groupby('critic')\n",
    "counts = grp.critic.count()  # number of reviews by each critic\n",
    "means = grp.fresh.mean()     # average freshness for each critic\n",
    "\n",
    "means[counts > 100].hist(bins=10, edgecolor='w', lw=1)\n",
    "plt.xlabel(\"Average rating per critic\")\n",
    "plt.ylabel(\"N\")\n",
    "plt.yticks([0, 2, 4, 6, 8, 10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building The Classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we create a [function](https://docs.python.org/2.7/tutorial/controlflow.html#defining-functions) that we will use below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# beginning of the function\n",
    "def make_xy(critics, vectorizer=None):\n",
    "    # create a vectorizer as we did before if none was passed to the function  \n",
    "    if vectorizer is None:\n",
    "        vectorizer = CountVectorizer()\n",
    "    # fit and transform in a combined function call\n",
    "    X = vectorizer.fit_transform(critics.quote)\n",
    "    X = X.tocsc()  # some versions of sklearn return COO format, basically a way to tidy up your array and to bring it to the format you are expecting\n",
    "    y = (critics.fresh == 'fresh').values.astype(np.int) #.values converts the return to an array, which is the converted to an integer\n",
    "    return X, y\n",
    "# end of the function\n",
    "\n",
    "# here, we call our function\n",
    "X, y = make_xy(critics)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Trainingskram erläutern"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MN Accuracy: 78.57%\n"
     ]
    }
   ],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.cross_validation import train_test_split\n",
    "xtrain, xtest, ytrain, ytest = train_test_split(X, y)\n",
    "clf = MultinomialNB().fit(xtrain, ytrain)\n",
    "print \"MN Accuracy: %0.2f%%\" % (100 * clf.score(xtest, ytest))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on training data: 0.92\n",
      "Accuracy on test data:     0.79\n"
     ]
    }
   ],
   "source": [
    "training_accuracy = clf.score(xtrain, ytrain)\n",
    "test_accuracy = clf.score(xtest, ytest)\n",
    "\n",
    "print \"Accuracy on training data: %0.2f\" % (training_accuracy)\n",
    "print \"Accuracy on test data:     %0.2f\" % (test_accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cross-Validation and hyper-parameter fitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.765118166871\n"
     ]
    }
   ],
   "source": [
    "from sklearn.cross_validation import KFold # KFold macht Cross Validation\n",
    "result = 0\n",
    "nfold = 5\n",
    "for train, test in KFold(y.size, nfold): # split data into train/test groups, 5 times\n",
    "    clf.fit(X[train], y[train]) # fit\n",
    "    result += clf.score(X[test], y[test]) # evaluate score function on held-out data\n",
    "print result / nfold # average"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def cv_score(clf, X, y, scorefunc):\n",
    "    result = 0.\n",
    "    nfold = 5\n",
    "    for train, test in KFold(y.size, nfold): # split data into train/test groups, 5 times\n",
    "        clf.fit(X[train], y[train]) # fit\n",
    "        result += scorefunc(clf, X[test], y[test]) # evaluate score function on held-out data\n",
    "    return result / nfold # average"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use the log-likelyhood as the score here. We'll go into this later..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def log_likelihood(clf, x, y):\n",
    "    prob = clf.predict_log_proba(x)\n",
    "    rotten = y == 0\n",
    "    fresh = ~rotten\n",
    "    return prob[rotten, 0].sum() + prob[fresh, 1].sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#the grid of parameters to search over\n",
    "alphas = [0, .1, 1, 5, 10, 50]\n",
    "min_dfs = [1e-5, 1e-4, 1e-3, 1e-2, 1e-1]\n",
    "\n",
    "#Find the best value for alpha and min_df, and the best classifier\n",
    "best_alpha = None\n",
    "best_min_df = None\n",
    "maxscore=-np.inf\n",
    "for alpha in alphas:\n",
    "    for min_df in min_dfs:         \n",
    "        vectorizer = CountVectorizer(min_df = min_df)       \n",
    "        Xthis, ythis = make_xy(critics, vectorizer)\n",
    "        \n",
    "        #your code here\n",
    "        clf = MultinomialNB(alpha=alpha)\n",
    "        cvscore = cv_score(clf, Xthis, ythis, log_likelihood)\n",
    "\n",
    "        if cvscore > maxscore:\n",
    "            maxscore = cvscore\n",
    "            best_alpha, best_min_df = alpha, min_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "alpha: 5.000000\n",
      "min_df: 0.001000\n"
     ]
    }
   ],
   "source": [
    "print \"alpha: %f\" % best_alpha\n",
    "print \"min_df: %f\" % best_min_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Work with the best params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on training data: 0.79\n",
      "Accuracy on test data:     0.74\n"
     ]
    }
   ],
   "source": [
    "vectorizer = CountVectorizer(min_df=best_min_df)\n",
    "X, Y = make_xy(critics, vectorizer)\n",
    "xtrain, xtest, ytrain, ytest = train_test_split(X, Y)\n",
    "\n",
    "clf = MultinomialNB(alpha=best_alpha).fit(xtrain, ytrain)\n",
    "\n",
    "# Your code here. Print the accuracy on the test and training dataset\n",
    "training_accuracy = clf.score(xtrain, ytrain)\n",
    "test_accuracy = clf.score(xtest, ytest)\n",
    "\n",
    "print \"Accuracy on training data: %0.2f\" % (training_accuracy)\n",
    "print \"Accuracy on test data:     %0.2f\" % (test_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 880  620]\n",
      " [ 402 1989]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "print confusion_matrix(ytest, clf.predict(xtest))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Interpretation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Good words\t     P(fresh | word)\n",
      "            touching 0.89\n",
      "             delight 0.89\n",
      "                rare 0.89\n",
      "         masterpiece 0.89\n",
      "             perfect 0.87\n",
      "          remarkable 0.87\n",
      "              modern 0.87\n",
      "        entertaining 0.87\n",
      "         intelligent 0.86\n",
      "              superb 0.86\n",
      "Bad words\t     P(fresh | word)\n",
      "              stupid 0.21\n",
      "      disappointment 0.20\n",
      "                dull 0.20\n",
      "             unfunny 0.20\n",
      "             muddled 0.20\n",
      "           pointless 0.18\n",
      "          uninspired 0.18\n",
      "               bland 0.16\n",
      "       unfortunately 0.15\n",
      "                lame 0.13\n"
     ]
    }
   ],
   "source": [
    "words = np.array(vectorizer.get_feature_names())\n",
    "\n",
    "x = np.eye(xtest.shape[1])\n",
    "probs = clf.predict_log_proba(x)[:, 0]\n",
    "ind = np.argsort(probs)\n",
    "\n",
    "good_words = words[ind[:10]]\n",
    "bad_words = words[ind[-10:]]\n",
    "\n",
    "good_prob = probs[ind[:10]]\n",
    "bad_prob = probs[ind[-10:]]\n",
    "\n",
    "print \"Good words\\t     P(fresh | word)\"\n",
    "for w, p in zip(good_words, good_prob):\n",
    "    print \"%20s\" % w, \"%0.2f\" % (1 - np.exp(p))\n",
    "    \n",
    "print \"Bad words\\t     P(fresh | word)\"\n",
    "for w, p in zip(bad_words, bad_prob):\n",
    "    print \"%20s\" % w, \"%0.2f\" % (1 - np.exp(p))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mis-predicted Rotten quotes\n",
      "---------------------------\n",
      "It survives today only as an unusually pure example of a typical 50s art-film strategy: the attempt to make the most modern and most popular of art forms acceptable to the intelligentsia by forcing it into an arcane, antique mold.\n",
      "\n",
      "The Waterboy is arguably Sandler's most enjoyable motion picture to date, but it's still far from a masterpiece.\n",
      "\n",
      "It is sometimes funny in a puzzling kind of way, it is generally overwrought in an irritating kind of way, and once in a while it is inappropriately touching.\n",
      "\n",
      "Benefits from a lively lead performance by the miscast Denzel Washington but doesn't come within light years of the book, one of the greatest American autobiographies.\n",
      "\n",
      "Herzog offers some evidence of Kinski's great human warmth, somewhat more of his rage of unimaginable proportions, and a good demonstration of Kinski's uncanny capacity to corkscrew his way into the frame.\n",
      "\n",
      "Mis-predicted Fresh quotes\n",
      "--------------------------\n",
      "A kind of insane logic seems to connect the sketches, if you look hard enough, but mostly the movie seems to exist in the present and be willing to try anything for a laugh.\n",
      "\n",
      "This tough-to-peg whodunit keeps you going for two hours, despite a few James Bond-ish (or Jane Bond-ish) turns that play less preposterously than you might assume were they to be divulged.\n",
      "\n",
      "The gangland plot is flimsy (bad guy Peter Greene wears too much eyeliner), and the jokes are erratic, but it's a far better showcase for Carrey's comic-from-Uranus talent than Ace Ventura.\n",
      "\n",
      "Though it's a good half hour too long, this overblown 1993 spin-off of the 60s TV show otherwise adds up to a pretty good suspense thriller.\n",
      "\n",
      "Some of the gags don't work, but fewer than in any previous Brooks film that I've seen, and when the jokes are meant to be bad, they are riotously poor. What more can one ask of Mel Brooks?\n",
      "\n"
     ]
    }
   ],
   "source": [
    "x, y = make_xy(critics, vectorizer)\n",
    "\n",
    "prob = clf.predict_proba(x)[:, 0]\n",
    "predict = clf.predict(x)\n",
    "\n",
    "bad_rotten = np.argsort(prob[y == 0])[:5]\n",
    "bad_fresh = np.argsort(prob[y == 1])[-5:]\n",
    "\n",
    "print \"Mis-predicted Rotten quotes\"\n",
    "print '---------------------------'\n",
    "for row in bad_rotten:\n",
    "    print critics[y == 0].quote.irow(row)\n",
    "    print\n",
    "\n",
    "print \"Mis-predicted Fresh quotes\"\n",
    "print '--------------------------'\n",
    "for row in bad_fresh:\n",
    "    print critics[y == 1].quote.irow(row)\n",
    "    print"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is a bad review! [ 0.01349276  0.98650724]\n"
     ]
    }
   ],
   "source": [
    "# check if the classifier will predict right\n",
    "result=clf.predict_proba(vectorizer.transform(['This movie is not remarkable, touching, or superb in any way']))\n",
    "if result[0][1]>result[0][0]:\n",
    "    print \"This is a bad review! \"+ str(result[0])\n",
    "else:\n",
    "    print \"This is a good review! \"+ str(result[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is a bad review! [ 0.03027171  0.96972829]\n"
     ]
    }
   ],
   "source": [
    "# check if the classifier will predict right\n",
    "result=clf.predict_proba(vectorizer.transform(['Remarkable and powerful.']))\n",
    "if result[0][1]>result[0][0]:\n",
    "    print \"This is a bad review! \"+ str(result[0])\n",
    "else:\n",
    "    print \"This is a good review! \"+ str(result[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What to Do Next?\n",
    "\n",
    "* tf-idf nutzen, mit Verweis auf Python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
